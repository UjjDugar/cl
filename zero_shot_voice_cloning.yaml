# Model
model_name: "amuvarma/3b-10m-pretrain-full"  # Replace with your base model must be compatible with the tokenizer and transformers library
tokenizer_name: "meta-llama/Llama-3.2-3B-Instruct"

# Training Args
epochs: 3
batch_size: 1
number_processes: 1
# pad_token: 128263
save_steps: 284
learning_rate: 5.0e-7

# Datasets
# text_QA_dataset: "amuvarma/text-messages-6m-processed-1-2g-8192l"
# TTS_dataset: "amuvarma/snac-10m-tts-combined"
training_dataset: "UjjD/zero_shot_proper_format" #"UjjD/tts_dataset_1.05M_padded_text_labels_on"

# Naming and paths
save_folder: "checkpoints"
project_name: "zero_shot_voice_cloning"
run_name: "lr 5e-7 text-tokens-on random-resampling-method"