{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import ElevenLabs, play\n",
    "\n",
    "client = ElevenLabs(api_key='sk_af7dc420ea22e0fe192d799fb6142bd14a731a0ea3e852d6')\n",
    "\n",
    "audio = client.text_to_speech.convert(\n",
    "    text=\"Dance my friends, dance!\",\n",
    "    voice_id='iCrDUkL56s3C8sCRl7wb',\n",
    "    model_id=\"eleven_multilingual_v2\",\n",
    "    output_format=\"mp3_44100_128\",\n",
    ")\n",
    "\n",
    "play(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file if available\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve your ElevenLabs API key from the environment\n",
    "api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"ELEVENLABS_API_KEY not set. Please set it in your environment or .env file.\")\n",
    "\n",
    "# Define the endpoint URL\n",
    "url = \"https://api.elevenlabs.io/v1/voices\"\n",
    "\n",
    "# Set up the request headers with your API key\n",
    "headers = {\n",
    "    \"xi-api-key\": api_key\n",
    "}\n",
    "\n",
    "# Make the GET request to the endpoint\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check for a successful response and parse the result\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    voices = data.get(\"voices\", [])\n",
    "    voice_ids = [voice.get(\"voice_id\") for voice in voices]\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def text_to_speech(voice_id, text, output_filename):\n",
    "    \"\"\"\n",
    "    Convert text to speech using ElevenLabs API and save to file\n",
    "    \n",
    "    Args:\n",
    "        voice_id (str): ElevenLabs voice ID to use\n",
    "        text (str): Text to convert to speech\n",
    "        output_filename (str): Filename to save the audio to\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"ELEVENLABS_API_KEY not set.\")\n",
    "\n",
    "    # API endpoint\n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
    "\n",
    "    # Request payload\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.75\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Headers\n",
    "    headers = {\n",
    "        \"xi-api-key\": api_key,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"audio/wav\" \n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        # print(f\"Audio saved to {output_filename}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sentences\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import soundfile as sf\n",
    "\n",
    "# Create speech_data directory if it doesn't exist\n",
    "os.makedirs('speech_data', exist_ok=True)\n",
    "\n",
    "with open('sentences_clean.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "# Initialize lists to store data\n",
    "voice_ids_list = []\n",
    "long_sentences = []\n",
    "long_sentence_audios = []\n",
    "short_sentences = []\n",
    "short_sentence_audios = []\n",
    "\n",
    "# For each voice, generate speech for first+last or second+second-last sentences etc\n",
    "for i, voice_id in tqdm(enumerate(voice_ids), total=len(voice_ids), desc=\"Generating speech\"):\n",
    "    # Get the appropriate sentences for this voice\n",
    "    long_sentence = sentences[i]\n",
    "    short_sentence = sentences[-(i+1)]\n",
    "    \n",
    "    # Generate speech for both sentences\n",
    "    long_path = os.path.join('speech_data', f\"voice{i}_long.wav\")\n",
    "    short_path = os.path.join('speech_data', f\"voice{i}_short.wav\")\n",
    "    \n",
    "    text_to_speech(voice_id, long_sentence, long_path)\n",
    "    text_to_speech(voice_id, short_sentence, short_path)\n",
    "    \n",
    "    # Read audio files\n",
    "    long_audio, _ = sf.read(long_path)\n",
    "    short_audio, _ = sf.read(short_path)\n",
    "    \n",
    "    # Store data\n",
    "    voice_ids_list.append(i+1)\n",
    "    long_sentences.append(long_sentence)\n",
    "    long_sentence_audios.append(long_audio)\n",
    "    short_sentences.append(short_sentence)\n",
    "    short_sentence_audios.append(short_audio)\n",
    "\n",
    "# Create dataset\n",
    "dataset_dict = {\n",
    "    'voice_id': voice_ids_list,\n",
    "    'long_sentence': long_sentences,\n",
    "    'long_sentence_audio': long_sentence_audios,\n",
    "    'short_sentence': short_sentences, \n",
    "    'short_sentence_audio': short_sentence_audios\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace Dataset and push to hub\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset.push_to_hub(\"UjjD/zero_shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.26s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.10s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:17<00:00,  8.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/UjjD/zero_shot/commit/480b9db290565b0a8b40617925354fb852861a3b', commit_message='Upload dataset', commit_description='', oid='480b9db290565b0a8b40617925354fb852861a3b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/UjjD/zero_shot', endpoint='https://huggingface.co', repo_type='dataset', repo_id='UjjD/zero_shot'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download dataset\n",
    "dataset = load_dataset(\"UjjD/zero_shot\")['train']\n",
    "\n",
    "# Add sample rate column\n",
    "dataset = dataset.add_column(\"sr\", [44100] * len(dataset))\n",
    "\n",
    "# Push updated dataset back to hub\n",
    "dataset.push_to_hub(\"UjjD/zero_shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 50/50 [00:01<00:00, 44.38 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long sentence: Attending to the heterogeneity of experience I suggest works against the biases of both anthropocentric thinking and conscious perception especially vision which tend to exaggerate the distance between inside and outside such theories proposing that regions of our own bodiesparts of usare more deeply immersed in and immediately involved with the outside world in which we participate may enable us to become more receptive to our entanglements with unloved others and perhaps more inclined to intervene on their behalf.\n",
      "Short sentence: Interestingly Massumi turns to Nietzsche to promote a concept of a subjectivity without a subject a doing without a doer.\n",
      "\n",
      "Audio files saved to first_long.wav and first_short.wav\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "dataset = load_dataset(\"UjjD/zero_shot\")['train']\n",
    "\n",
    "# Get first row\n",
    "first_row = dataset[0]\n",
    "\n",
    "# Display the text\n",
    "print(\"Long sentence:\", first_row['long_sentence'])\n",
    "print(\"Short sentence:\", first_row['short_sentence'])\n",
    "\n",
    "# Save the audio files\n",
    "long_audio_path = \"first_long.wav\"\n",
    "short_audio_path = \"first_short.wav\"\n",
    "\n",
    "sf.write(long_audio_path, first_row['long_sentence_audio'], first_row['sr'])\n",
    "sf.write(short_audio_path, first_row['short_sentence_audio'], first_row['sr'])\n",
    "\n",
    "print(f\"\\nAudio files saved to {long_audio_path} and {short_audio_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from cartesia import Cartesia\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def text_to_speech_cartesia(voice_id, text, output_filename):\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    if os.environ.get(\"CARTESIA_API_KEY\") is None:\n",
    "        raise ValueError(\"CARTESIA_API_KEY is not set\")\n",
    "\n",
    "    client = Cartesia(api_key=os.environ.get(\"CARTESIA_API_KEY\"))\n",
    "\n",
    "    data = client.tts.bytes(\n",
    "        model_id=\"sonic\",\n",
    "        transcript=text,\n",
    "        voice_id=voice_id,\n",
    "        output_format={\n",
    "            \"container\": \"wav\",\n",
    "            \"encoding\": \"pcm_f32le\",\n",
    "            \"sample_rate\": 44100,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_voice_id = 'a3afd376-04f9-48e2-a966-132cdfdbc093'\n",
    "\n",
    "# Read first sentence from sentences_clean.txt\n",
    "with open('sentences_clean.txt', 'r') as f:\n",
    "    first_sentence = f.readline().strip()\n",
    "\n",
    "# Generate speech using example voice\n",
    "text_to_speech_cartesia(\n",
    "    voice_id=example_voice_id,\n",
    "    text=first_sentence, \n",
    "    output_filename='bla_bla.wav'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartesia import Cartesia\n",
    "import os\n",
    "\n",
    "client = Cartesia(api_key=os.environ.get(\"CARTESIA_API_KEY\"))\n",
    "\n",
    "# Get all available voices\n",
    "voices = client.voices.list()\n",
    "#print(voices)\n",
    "\n",
    "# Extract voice IDs into a list\n",
    "voice_ids = [voice[\"id\"] for voice in voices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sentences\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import soundfile as sf\n",
    "\n",
    "# Create speech_data directory if it doesn't exist\n",
    "os.makedirs('speech_data', exist_ok=True)\n",
    "\n",
    "with open('sentences_clean.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "\n",
    "# Initialize lists to store data\n",
    "voice_ids_list = []\n",
    "long_sentences = []\n",
    "long_sentence_audios = []\n",
    "short_sentences = []\n",
    "short_sentence_audios = []\n",
    "sample_rates = []\n",
    "\n",
    "# Load existing dataset\n",
    "existing_dataset = load_dataset(\"UjjD/zero_shot\")[\"train\"]\n",
    "\n",
    "# For each voice, generate speech for 51st+51st-last or 52nd+52nd-last sentences etc\n",
    "for i, voice_id in tqdm(enumerate(voice_ids), total=len(voice_ids), desc=\"Generating speech\"):\n",
    "    # Get the appropriate sentences for this voice (starting from 51st sentence)\n",
    "    long_sentence = sentences[50 + i]\n",
    "    short_sentence = sentences[-(51 + i)]\n",
    "    \n",
    "    # Generate speech for both sentences\n",
    "    long_path = os.path.join('speech_data', f\"voice{50+i}_long.wav\")\n",
    "    short_path = os.path.join('speech_data', f\"voice{50+i}_short.wav\")\n",
    "    \n",
    "    text_to_speech_cartesia(voice_id, long_sentence, long_path)\n",
    "    text_to_speech_cartesia(voice_id, short_sentence, short_path)\n",
    "    \n",
    "    # Read audio files\n",
    "    long_audio, sr_long = sf.read(long_path)\n",
    "    short_audio, sr_short = sf.read(short_path)\n",
    "    \n",
    "    # Store data\n",
    "    voice_ids_list.append(50 + i + 1)  # Start voice IDs from 51\n",
    "    long_sentences.append(long_sentence)\n",
    "    long_sentence_audios.append(long_audio)\n",
    "    short_sentences.append(short_sentence)\n",
    "    short_sentence_audios.append(short_audio)\n",
    "    sample_rates.append(sr_long)  # Using long audio's sample rate (should be same for both)\n",
    "\n",
    "# Create dataset\n",
    "new_dataset_dict = {\n",
    "    'voice_id': voice_ids_list,\n",
    "    'long_sentence': long_sentences,\n",
    "    'long_sentence_audio': long_sentence_audios,\n",
    "    'short_sentence': short_sentences, \n",
    "    'short_sentence_audio': short_sentence_audios,\n",
    "    'sr': sample_rates\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace Dataset and concatenate with existing\n",
    "new_dataset = Dataset.from_dict(new_dataset_dict)\n",
    "combined_dataset = concatenate_datasets([existing_dataset, new_dataset])\n",
    "combined_dataset.push_to_hub(\"UjjD/zero_shot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
